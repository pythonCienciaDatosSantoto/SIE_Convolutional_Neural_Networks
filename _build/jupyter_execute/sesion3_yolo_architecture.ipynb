{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Objetos en Tiempo Real con YOLO\n",
    "\n",
    "**Objetivo:** Después de entender la teoría detrás de los detectores de dos pasos como R-CNN, en este notebook nos sumergiremos en el paradigma que revolucionó la detección de objetos: YOLO (You Only Look Once). Explicaremos su arquitectura y, lo más importante, usaremos un modelo YOLOv8 pre-entrenado para detectar objetos en varias imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. YOLO: El Cambio a Detectores de Un Solo Paso\n",
    "\n",
    "Como vimos, la familia R-CNN, aunque efectiva, sufría de un problema de velocidad por su naturaleza de dos pasos (proponer regiones y luego clasificarlas). YOLO, propuesto por Joseph Redmon et al., cambió las reglas del juego al tratar la detección de objetos como un **único problema de regresión**.\n",
    "\n",
    "**¿Cómo funciona?**\n",
    "\n",
    "1.  **Divide la imagen en una cuadrícula (Grid):** La imagen de entrada se divide en una cuadrícula (ej. S x S).\n",
    "2.  **Predicción por Celda:** Cada celda de la cuadrícula es responsable de predecir si el centro de un objeto cae dentro de ella. Si es así, la celda predice:\n",
    "    -   Los **cuadros delimitadores (bounding boxes)** para ese objeto.\n",
    "    -   Un **puntaje de confianza** (qué tan seguro está de que hay un objeto).\n",
    "    -   Las **probabilidades de clase** (qué tipo de objeto es).\n",
    "3.  **Una Sola Pasada:** Todo esto se logra en una única pasada de la imagen a través de la red neuronal, lo que hace a YOLO extremadamente rápido.\n",
    "\n",
    "![Diagrama conceptual de YOLO](images/yolo_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. La Arquitectura de YOLO\n",
    "\n",
    "Un modelo YOLO moderno típicamente se divide en tres partes:\n",
    "\n",
    "- **Backbone (Columna Vertebral):** Es una red convolucional profunda (como una versión de ResNet o CSPDarknet) que actúa como un extractor de características.\n",
    "- **Neck (Cuello):** Conecta el Backbone con el Head, combinando mapas de características de diferentes escalas para detectar objetos grandes y pequeños.\n",
    "- **Head (Cabeza):** Realiza las predicciones finales: cuadros delimitadores, confianzas y clases.\n",
    "\n",
    "Ahora, ¡manos a la obra!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Práctica: Detección de Objetos con YOLOv8\n",
    "\n",
    "Usaremos la librería `ultralytics` y un modelo **YOLOv8** pre-entrenado en el dataset COCO (80 clases de objetos comunes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Importar librerías\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Cargar un modelo YOLOv8 pre-entrenado\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Función Auxiliar para Detección y Visualización\n",
    "\n",
    "Para no repetir código, crearemos una función que tome una URL de una imagen, realice la detección y la muestre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_show(image_url):\n",
    "    \"\"\"Toma la URL de una imagen, la procesa con YOLO y muestra los resultados.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "        results = model(image) # Realizar la detección\n",
    "        \n",
    "        for r in results:\n",
    "            im_array = r.plot()  # Dibuja los cuadros y etiquetas\n",
    "            im = Image.fromarray(im_array[..., ::-1]) # Convierte de BGR a RGB\n",
    "        \n",
    "        # Mostrar la imagen con las detecciones\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo procesar la imagen desde {image_url}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Probando el Modelo con Múltiples Ejemplos\n",
    "\n",
    "Ahora, usemos nuestra función con una lista de imágenes para ver qué tan versátil es el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Procesando imagen: https://ultralytics.com/images/bus.jpg... ---\n"
     ]
    }
   ],
   "source": [
    "image_urls = [\n",
    "    # Ejemplo 1: Escena urbana (el original)\n",
    "    'https://ultralytics.com/images/bus.jpg',\n",
    "    # Ejemplo 2: Escena deportiva\n",
    "    'https://ultralytics.com/images/zidane.jpg',\n",
    "    # Ejemplo 3: Escena de comida\n",
    "    'https://images.pexels.com/photos/1099680/pexels-photo-1099680.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1',\n",
    "    # Ejemplo 4: Escena interior\n",
    "    'https://images.pexels.com/photos/1571460/pexels-photo-1571460.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1',\n",
    "    # Ejemplo 5: Escena de tráfico\n",
    "    'https://images.pexels.com/photos/3785931/pexels-photo-3785931.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1'\n",
    "]\n",
    "\n",
    "for url in image_urls:\n",
    "    print(f\"--- Procesando imagen: {url[:50]}... ---\")\n",
    "    detect_and_show(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explicación de los Resultados\n",
    "\n",
    "¡Los resultados son impresionantes! Con solo unas pocas líneas de código, hemos localizado y clasificado múltiples objetos en una variedad de escenas. Analicemos lo que vemos en los ejemplos:\n",
    "\n",
    "- **Cuadros Delimitadores (Bounding Boxes):** Cada cuadro de color encierra un objeto que el modelo ha detectado.\n",
    "- **Etiqueta de Clase:** Encima de cada cuadro, vemos la clase predicha (ej. `person`, `bus`, `car`, `bowl`, `chair`).\n",
    "- **Puntaje de Confianza:** Junto a la etiqueta, hay un número (ej. `0.89`). Esta es la confianza que el modelo tiene en su propia predicción. Un valor cercano a 1.0 significa que el modelo está muy seguro.\n",
    "\n",
    "El modelo demuestra su capacidad para identificar correctamente personas en diferentes contextos, vehículos, comida y muebles, probando la eficacia de YOLO para analizar escenas complejas y variadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusión del Curso\n",
    "\n",
    "¡Felicidades! A lo largo de estas tres sesiones, hemos realizado un viaje completo:\n",
    "\n",
    "1.  Comenzamos con **redes neuronales densas** y descubrimos por qué no son adecuadas para imágenes.\n",
    "2.  Introdujimos las **Redes Neuronales Convolucionales (CNNs)**, construyendo modelos desde cero y explorando arquitecturas clásicas como LeNet-5 y AlexNet.\n",
    "3.  Finalmente, dimos el salto a la **detección de objetos**, entendiendo los conceptos de R-CNN y utilizando un modelo de última generación como **YOLO** para una tarea del mundo real.\n",
    "\n",
    "Este es solo el comienzo del fascinante mundo de la visión por computadora. Ahora tienes el conocimiento fundamental para entender cómo funcionan estas poderosas herramientas y cómo empezar a aplicarlas en tus propios proyectos. ¡Excelente trabajo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}